{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# NLTK VADER for sentiment analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "df = table[0]\n",
    "df_sector = df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up some data\n",
    "list = df.replace({'BRK.B':'BRK-B', 'BF.B':'BF-B'})\n",
    "indexNames = list[ list['Symbol'] == 'OGN' ].index\n",
    "list.drop(indexNames , inplace=True)\n",
    "newlist = list['Symbol'].to_numpy()\n",
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import URLError, HTTPError\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "news_tables = {}\n",
    "tickers = newlist\n",
    "finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "for ticker in tickers:\n",
    "    url = finwiz_url + ticker\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'}) \n",
    "    try:\n",
    "      response = urlopen(req)  \n",
    "\n",
    "    except URLError as e:\n",
    "      print(url)\n",
    "    # else:\n",
    "      # Process data get from the URL opened\n",
    "      # If an exception has been catch, you won't \n",
    "      # enter in this else block\n",
    "      # print(\"No problem!\")\n",
    "\n",
    "    # Read the contents of the file into 'html'\n",
    "    html = BeautifulSoup(response)\n",
    "    # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "    news_table = html.find(id='news-table')\n",
    "    # Add the table to our dictionary\n",
    "    news_tables[ticker] = news_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one single day of headlines for 'AMZN' \n",
    "amzn = news_tables['AMZN']\n",
    "# Get all the table rows tagged in HTML with <tr> into 'amzn_tr'\n",
    "amzn_tr = amzn.findAll('tr')\n",
    "\n",
    "for i, table_row in enumerate(amzn_tr):\n",
    "    # Read the text of the element 'a' into 'link_text'\n",
    "    a_text = table_row.a.text\n",
    "    # Read the text of the element 'td' into 'data_text'\n",
    "    td_text = table_row.td.text\n",
    "    # Print the contents of 'link_text' and 'data_text'\n",
    "    hyperlink = table_row.a.get('href') \n",
    "    print(a_text)\n",
    "    print(td_text)\n",
    "    print(hyperlink)\n",
    "    # Exit after printing 4 rows of data\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_news = []\n",
    "\n",
    "# Iterate through the news\n",
    "for file_name, news_table in news_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # read the text from each tr tag into text\n",
    "        # get text from a only\n",
    "        text = x.a.get_text() \n",
    "        # splite text in the td tag into a list \n",
    "        date_scrape = x.td.text.split()\n",
    "        # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "            \n",
    "        # else load 'date' as the 1st element and 'time' as the second    \n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "        # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "        ticker = file_name.split('_')[0]\n",
    "        hyperlink = x.a.get('href')\n",
    "        \n",
    "        # Append ticker, date, time and headline, news URL as a list to the 'parsed_news' list\n",
    "        parsed_news.append([ticker, date, time, text, hyperlink])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline', 'hyperlink']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "\n",
    "parsed_and_scored_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = pathlib.Path(\".\")\n",
    "# filename = \"News_SP500_Sentiment.csv\"\n",
    "# filepath = folder / filename\n",
    "parsed_and_scored_news.to_csv('News_SP500_Sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date and ticker columns from scored_news and calculate the mean\n",
    "mean_scores = parsed_and_scored_news.groupby(['ticker','date']).mean()\n",
    "# Unstack the column ticker\n",
    "mean_scores = mean_scores.unstack()\n",
    "# Get the cross-section of compound in the 'columns' axis\n",
    "mean_scores = mean_scores.xs('compound', axis=\"columns\").transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "# mean_scores\n",
    "# Plot a bar chart with pandas\n",
    "exmaple = mean_scores['AMZN']\n",
    "exmaple.plot(kind = 'line',x='date',y='mean_scores',color='red')\n",
    "plt.grid()"
   ]
  }
 ]
}